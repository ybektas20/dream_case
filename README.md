I used notebooks as reports, with numerous charts and detailed explanations in markdown cells. For a detailed analysis, please review the provided notebooks. Here is a summary of the project:


For Question 1, the analysis began with a thorough data loading and preprocessing phase where each table was examined for data completeness and consistency. The install, level_end, session, revenue, and cost tables were all checked, and while a few null values were present in the level_end and session data (likely due to network or user issues), the overall quality was high. In examining user acquisition, it was found that Royal Match registered a median of 6,810 installs per day (excluding the low-data first and last days), with daily installations ranging from 4,859 to 9,731. Notably, there was a distinct spike in installs for a specific segment—observed for Pluton, Android users—on days where marketing expenditures varied; for instance, on May 19 there was zero expenditure with 1,030 installs, followed by days with substantial spend ($2,149 on May 20 and $4,053 on May 21) corresponding to 3,815 and 4,428 installs respectively. The analysis of daily active users (DAU) revealed that activity peaked at 80,922 on May 30 and then declined to 62,014 by June 14, a pattern that aligns with a reduction in ad spending which dropped to zero after June 1. In terms of engagement and retention, the 1-day and 7-day retention rates averaged around 55%, with a slight downtrend observed starting on May 26. Furthermore, iOS users consistently exhibited higher retention than Android users. Gameplay analysis showed that average time spent per user per day was approximately 857 seconds, with considerable variation by segment: after May 12, users from the Uranus segment spent the most time, Android users generally logged more time than iOS users, and those from the Sid network were particularly engaged. Temporal patterns were also interesting—contrary to common expectations, the game was most played on Tuesdays and least on Fridays, and there was about a 250-second difference in average time spent across different hours of the day, likely reflecting time zone differences. In addition, a deeper dive into level progression uncovered that while total moves per level ranged from 5 to 89 (possibly due to in-game purchases of extra moves), certain levels (e.g., level 199) were notably challenging, with an average of seven attempts needed to pass, suggesting that some levels may serve more as introductions to new gameplay concepts. Cost analysis showed that the daily expenditure, averaging around 30,668, started at a high level and then declined gradually until it reached zero on June 1, marking the end of the marketing campaign. Spending differences were evident by platform and network, with iOS receiving roughly twice the expenditure of Android and the Buzz network being eight times higher than the next highest channel. A regression analysis of cost against new installs indicated that, on average, each unit of cost contributed an increase of about 0.16 new users, although segmented analyses revealed significant variability in cost efficiency across different groups. On the monetization side, total revenue trended upward until a peak on June 5 and then declined, which coincided with the end of marketing. Most revenue was driven by the Mercury, iOS, Buzz segment—especially from the “lovely_pack” package—while daily profit remained negative on average. The ARPU calculated for May 31 was 0.48, and the ARPDau on June 14 was approximately 0.06, highlighting that while initial user acquisition was moderately profitable, ongoing engagement and revenue generation suffered following the drop in marketing spend. Overall, the findings for Question 1 underscore the critical influence of marketing on user acquisition and engagement, while also revealing challenges in sustaining long-term monetization once marketing efforts cease.


In Question 2, the focus shifted to analyzing the performance of an AB test that compared two variants (Group A and Group B) using three tables capturing test entries, revenue, and session data. Initial checks confirmed that the users were evenly distributed across the groups and platforms, ensuring a fair test. Engagement metrics revealed that Group A users, regardless of platform, spent more time in the game, had more sessions, and progressed to higher levels compared to Group B. Retention analyses (both 1-day and 7-day) consistently showed higher retention for Group A, especially among iOS players. However, when examining monetization, the results painted a different picture. Despite lower engagement, Group B exhibited a significantly higher conversion rate (8.99% versus 7.48%) and a higher ARPU (with mean revenue per user around $15.25 compared to $11.82 in Group A). Statistical tests confirmed that these differences were significant, suggesting that Group B’s variant was more effective at driving purchases. Further revenue analysis, including the evaluation of package pricing and a Lorenz curve comparison, indicated that while most users did not make purchases, the paying users in Group B contributed more evenly to total revenue. The pricing strategy differed modestly between the groups, with Group A tending to price mix packs slightly higher than Group B. Overall, the AB test revealed a trade-off: Group A’s variant generated deeper engagement, whereas Group B’s variant, likely due to more effective in-game purchase prompts or personalized offers, delivered superior monetization. The findings suggest that future strategies should consider integrating the strong monetization tactics of Group B into the more engaging framework of Group A to achieve a better overall balance between engagement and revenue generation.


For Question 3, the analysis centered on predictive modeling to understand user conversion, with an emphasis on feature engineering, transformation, and calibration. The dataset was enriched with a variety of baseline features (such as age, time_spent, and level_success) and a host of engineered metrics designed to capture user behavior, spending patterns, and engagement ratios. To handle the skewed nature of many features, multiple scalers were compared, with the PowerTransformer emerging as the most effective in normalizing the data and enhancing probability calibration for subsequent linear models. Correlation analyses revealed that most numeric features (excluding age) showed meaningful relationships with the target, while conditional distribution plots indicated that some features (like time_spent and level_success) had a strong linear discriminatory power, whereas others (such as net_booster and booster_spend_ratio) exhibited non-linear relationships. To capture these non-linear effects, quadratic transformations were applied to net_booster and net_coin, and these enhanced features proved useful in subsequent model training. The modeling phase involved the construction of multiple pipelines based on Elastic-Net logistic regression and XGBoost, both with and without sample weighting, as well as calibration using isotonic regression. Given the data imbalance, sample weights based on inverse class frequencies were used to improve model learning. Extensive hyperparameter tuning was performed using RandomizedSearchCV and GridSearchCV, and models were evaluated using ROC AUC, F1 score, and Brier score, along with calibration and ROC curves. Among all models tested, the XGB_Weighted_Calibrated model demonstrated the best performance, with the highest ROC AUC (0.8680) and the best Brier score (0.0562), indicating strong discrimination and reliable probability estimates. An additional experiment with an RBF kernel-like transformation did not yield significant improvements, suggesting that the original engineered features were already capturing the necessary signal. The final conclusions for Question 3 emphasize the importance of proper feature transformation and calibration, particularly in imbalanced settings. The calibrated XGB model is recommended for predicting user conversion, and its probability estimates can be leveraged for dynamic in-game pricing, personalized reward systems, and targeted retention strategies.

In summary, the combined analysis across all three questions provides a comprehensive view of Royal Match’s performance. The first analysis highlights the critical role of marketing in driving user acquisition, engagement, and revenue, while also revealing challenges in sustaining these metrics once marketing efforts cease. The second analysis of the AB test shows that while deeper user engagement can be achieved through one variant, superior monetization may require different approaches, suggesting that a hybrid strategy could yield the best overall performance. Finally, the predictive modeling work in Question 3 demonstrates that careful feature engineering, transformation, and model calibration are essential for building reliable conversion models in an imbalanced environment. Together, these insights suggest strategic recommendations for reallocating marketing spend toward high-efficiency segments, integrating effective monetization tactics, and leveraging advanced predictive models to optimize in-game offers and user retention.